{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data files\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file youtoxic_english_1000.csv to youtoxic dataframe\n",
    "youtoxic = pd.read_csv('data/youtoxic_english_1000.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\nDont you reckon them 'black lives matter' ba...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\nDont you reckon them 'black lives matter' ba...     True       True   \n",
       "3  There are a very large number of people who do...    False      False   \n",
       "4  The Arab dude is absolutely right, he should h...    False      False   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0     False          False      False         False     False          False   \n",
       "1     False          False      False         False     False          False   \n",
       "2     False          False       True         False     False          False   \n",
       "3     False          False      False         False     False          False   \n",
       "4     False          False      False         False     False          False   \n",
       "\n",
       "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0     False         False            False         False  \n",
       "1     False         False            False         False  \n",
       "2     False         False            False         False  \n",
       "3     False         False            False         False  \n",
       "4     False         False            False         False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtoxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   CommentId        1000 non-null   object\n",
      " 1   VideoId          1000 non-null   object\n",
      " 2   Text             1000 non-null   object\n",
      " 3   IsToxic          1000 non-null   bool  \n",
      " 4   IsAbusive        1000 non-null   bool  \n",
      " 5   IsThreat         1000 non-null   bool  \n",
      " 6   IsProvocative    1000 non-null   bool  \n",
      " 7   IsObscene        1000 non-null   bool  \n",
      " 8   IsHatespeech     1000 non-null   bool  \n",
      " 9   IsRacist         1000 non-null   bool  \n",
      " 10  IsNationalist    1000 non-null   bool  \n",
      " 11  IsSexist         1000 non-null   bool  \n",
      " 12  IsHomophobic     1000 non-null   bool  \n",
      " 13  IsReligiousHate  1000 non-null   bool  \n",
      " 14  IsRadicalism     1000 non-null   bool  \n",
      "dtypes: bool(12), object(3)\n",
      "memory usage: 35.3+ KB\n"
     ]
    }
   ],
   "source": [
    "youtoxic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir Booleanos a Int64 por si alguna librería posterior no puede hacer la transformación intrínseca de boolean a int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   CommentId        1000 non-null   object\n",
      " 1   VideoId          1000 non-null   object\n",
      " 2   Text             1000 non-null   object\n",
      " 3   IsToxic          1000 non-null   int64 \n",
      " 4   IsAbusive        1000 non-null   int64 \n",
      " 5   IsThreat         1000 non-null   int64 \n",
      " 6   IsProvocative    1000 non-null   int64 \n",
      " 7   IsObscene        1000 non-null   int64 \n",
      " 8   IsHatespeech     1000 non-null   int64 \n",
      " 9   IsRacist         1000 non-null   int64 \n",
      " 10  IsNationalist    1000 non-null   int64 \n",
      " 11  IsSexist         1000 non-null   int64 \n",
      " 12  IsHomophobic     1000 non-null   int64 \n",
      " 13  IsReligiousHate  1000 non-null   int64 \n",
      " 14  IsRadicalism     1000 non-null   int64 \n",
      "dtypes: int64(12), object(3)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Identify boolean columns\n",
    "bool_columns = youtoxic.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert boolean columns to int64\n",
    "youtoxic[bool_columns] = youtoxic[bool_columns].astype('int64')\n",
    "\n",
    "# Display the updated information about the dataset to verify the conversion\n",
    "youtoxic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the youtoxic dataframe to an Excel file\n",
    "#youtoxic.to_excel('youtoxic.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Después de analizar el fichero en Excel se tienen estas conclusiones:\n",
    "\n",
    "* Efectivamente IsToxic es la bandera que agrupa a los diferentes tipos de clasificaciones de odio.\n",
    "* No hay ningún discurso de odio que no tenga la bandera IsToxic encendida.\n",
    "* No hay ninguna bandera IsToxic encendida sin que haya ninguna de las otras banderas encendidas. Esto significa que la clasificación IsToxic no existe per se y solo representa la existencia de discurso de odio en alguna de las características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje de registros en cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Count of 1s</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IsToxic</td>\n",
       "      <td>462</td>\n",
       "      <td>46.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IsAbusive</td>\n",
       "      <td>353</td>\n",
       "      <td>35.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IsThreat</td>\n",
       "      <td>21</td>\n",
       "      <td>2.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IsProvocative</td>\n",
       "      <td>161</td>\n",
       "      <td>16.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IsObscene</td>\n",
       "      <td>100</td>\n",
       "      <td>10.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IsHatespeech</td>\n",
       "      <td>138</td>\n",
       "      <td>13.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IsRacist</td>\n",
       "      <td>125</td>\n",
       "      <td>12.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IsNationalist</td>\n",
       "      <td>8</td>\n",
       "      <td>0.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IsSexist</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IsHomophobic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IsReligiousHate</td>\n",
       "      <td>12</td>\n",
       "      <td>1.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IsRadicalism</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Column  Count of 1s Percentage\n",
       "0           IsToxic          462     46.20%\n",
       "1         IsAbusive          353     35.30%\n",
       "2          IsThreat           21      2.10%\n",
       "3     IsProvocative          161     16.10%\n",
       "4         IsObscene          100     10.00%\n",
       "5      IsHatespeech          138     13.80%\n",
       "6          IsRacist          125     12.50%\n",
       "7     IsNationalist            8      0.80%\n",
       "8          IsSexist            1      0.10%\n",
       "9      IsHomophobic            0      0.00%\n",
       "10  IsReligiousHate           12      1.20%\n",
       "11     IsRadicalism            0      0.00%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get total number of rows\n",
    "total_rows = len(youtoxic)\n",
    "\n",
    "# Select integer columns\n",
    "int_cols = youtoxic.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Calculate counts and percentages\n",
    "results = []\n",
    "for col in int_cols:\n",
    "    count_ones = youtoxic[col].sum()  # Sum of 1s\n",
    "    percentage = (count_ones / total_rows) * 100\n",
    "    results.append({\n",
    "        'Column': col,\n",
    "        'Count of 1s': count_ones,\n",
    "        'Percentage': f'{percentage:.2f}%'\n",
    "    })\n",
    "\n",
    "# Create and display table\n",
    "table = pd.DataFrame(results)\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: IsToxic es la columna bandera que indica si hay algún comentario de odio en las otras 11 categorias. No se necesita crear una columna que sumarice 12 columnas porque esa columna ya está en el dataset.\n",
    "\n",
    "* Los datos no nos permiten clasificar un discurso por medio de las características: - \n",
    "- IsHomophobic\n",
    "- IsRadicalism\n",
    "\n",
    "* Las características \n",
    "- IsSexist\n",
    "- IsNationalist\n",
    "- IsReligiousHate\n",
    "- IsThreat \n",
    "están severamente desbalanceadas.\n",
    "\n",
    "* También están muy desbalanceadas las características: \n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "* La única característica donde el desbalanceo es razonable es:\n",
    "- IsAbusive\n",
    "\n",
    "* Con estas observaciones se presume que el modelo con una sola bandera IsToxic es muy generalista y que por lo tanto no es capaz de predecir con una mejor precisión (alrededor de 70%) si el mensaje es de odio o no.\n",
    "\n",
    "* Un modelo multi-label binary classification (entiendo que con un Naive Bayes) puede ser una mejor solución en este caso, utilizando solamente las características:\n",
    "- IsAbusive\n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "Por supuesto, resolviendo el problema del desbalanceo en las últimas 4 características.\n",
    "\n",
    "Después de reconsiderarlo, se descarta la caracteristica VideoId porque aunque sí puede aportar información de un vídeo, el hecho de que la muestra es tan pequeña y seguramente estos vídeos no se utlizarán al momento de predecir hace que no sea útil la información que pueda aportar aunque en las pruebas iniciales sí haya mejorado la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A partir de aquí se preparan 2 datasets. Uno para entrenar un modelo multi-etiqueta de clasificación binaria. El otro para un modelo de una sola categoria: Istoxic\n",
    "\n",
    "Youmultihatred:\n",
    "- Text\n",
    "- IsAbusive\n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "Youtoxic:\n",
    "- Text\n",
    "- IsToxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep for version without multi-label classification\n",
    "\n",
    "#youmultihatred = youtoxic[['Text', 'IsAbusive','IsProvocative','IsObscene','IsHatespeech', 'IsRacist']]\n",
    "\n",
    "youtoxic = youtoxic[['Text', 'IsToxic']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess dataset\n",
    "\n",
    "1. Remover URLs\n",
    "2. Remover special characters y números\n",
    "3. Convertir a minúsculas\n",
    "4. Remover espacios innecesarios\n",
    "Tokenizar momentaneamente para:\n",
    "5. Quitar Stopwords\n",
    "6. Lematizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people would take step back make case wasnt an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont reckon black life matter banner held whit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large number people like police officer called...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  IsToxic\n",
       "0  people would take step back make case wasnt an...        0\n",
       "1  law enforcement trained shoot apprehend traine...        1\n",
       "2  dont reckon black life matter banner held whit...        1\n",
       "3  large number people like police officer called...        0\n",
       "4  arab dude absolutely right shot extra time sho...        0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# No utilizaremos textblob dado que las metricas mejoraron al no utilizarlo.\n",
    "# Ademas, textblob es muy lento para procesar grandes cantidades de texto.\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Correct misspellings\n",
    "    #text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the preprocess_text function to the 'Text' column of the youtoxic and youmultihatred dataframes\n",
    "youtoxic['Text'] = youtoxic['Text'].apply(preprocess_text)\n",
    "#youmultihatred['Text'] = youmultihatred['Text'].apply(preprocess_text)\n",
    "\n",
    "# Display the updated dataframes\n",
    "youtoxic.head()\n",
    "#youmultihatred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sample csv file from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a random selection of 20 rows from youtoxic dataframe to a CSV file\n",
    "# Uncomment the line below to export the sample\n",
    "\n",
    "youtoxic.head(n=20).to_csv('data/youtoxic_sample.csv', index=False)\n",
    "#youmultihatred.head(n=20).to_csv('data/youmultihatred_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Bert Tokenizer + PCA reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-56' coro=<Server.serve() done, defined at c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\uvicorn\\server.py:67> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\uvicorn\\main.py\", line 577, in run\n",
      "    server.run()\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\uvicorn\\server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\asyncio\\tasks.py\", line 396, in __wakeup\n",
      "    self.__step()\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\asyncio\\tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\uvicorn\\server.py\", line 68, in serve\n",
      "    with self.capture_signals():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"c:\\Users\\jvazq\\Data_Analysis_Tools\\anaconda3\\envs\\airbnb_analytics\\Lib\\site-packages\\uvicorn\\server.py\", line 328, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: 0.71 (+/- 0.07)\n",
      "Training Accuracy: 0.78\n",
      "Test Accuracy: 0.74\n",
      "Overfitting Percentage: 4.33%\n",
      "F1-score: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import skl2onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.algebra.onnx_ops import OnnxConcat\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Initialize BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(texts, max_length=128):\n",
    "    # Tokenize and encode sequences\n",
    "    tokens = tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokens)\n",
    "        # Use [CLS] token embeddings as sequence representation\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Prepare data\n",
    "X = youtoxic['Text']\n",
    "y = youtoxic['IsToxic']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_train_bert = get_bert_embeddings(X_train)\n",
    "X_test_bert = get_bert_embeddings(X_test)\n",
    "\n",
    "# 1. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bert)\n",
    "X_test_scaled = scaler.transform(X_test_bert)\n",
    "\n",
    "# 2. Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.8)  # Keep 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# 3. Stronger regularization in LogisticRegression\n",
    "model = LogisticRegression(\n",
    "    C=0.1,  # Increased regularization strength\n",
    "    penalty='elasticnet',  # Combine L1 and L2\n",
    "    solver='saga',\n",
    "    l1_ratio=0.5,  # Equal mix of L1 and L2\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# 4. Add cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_pca, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})\")\n",
    "\n",
    "# 5. Train and evaluate\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_train_pred = model.predict(X_train_pca)\n",
    "y_test_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "overfitting_percentage = ((train_accuracy - test_accuracy) / train_accuracy) * 100\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Overfitting Percentage: {overfitting_percentage:.2f}%')\n",
    "print(f'F1-score: {f1_score(y_test, y_test_pred):.2f}')\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'model_runtimes/best_model.pkl')\n",
    "joblib.dump(pca, 'model_runtimes/pca_model.pkl')\n",
    "joblib.dump(scaler, 'model_runtimes/scaler_model.pkl')\n",
    "\n",
    "# Define the pipeline with scaler, PCA, and logistic regression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('pca', pca),\n",
    "    ('logistic_regression', model)\n",
    "])\n",
    "\n",
    "# Convert the pipeline to ONNX format\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train_bert.shape[1]]))]\n",
    "onnx_model = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "with open(\"model_runtimes/pipeline_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building an API to consume the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 FastAPI for Bert + PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5212]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [5212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API is running at http://127.0.0.1:8000\n",
      "Documentation available at http://127.0.0.1:8000/docs\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import RedirectResponse\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "import joblib\n",
    "\n",
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Load BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the trained model\n",
    "best_model = joblib.load('model_runtimes/best_model.pkl')\n",
    "\n",
    "# Load PCA and scaler\n",
    "pca = joblib.load('model_runtimes/pca_model.pkl')\n",
    "scaler = joblib.load('model_runtimes/scaler_model.pkl')\n",
    "\n",
    "def get_bert_embeddings(text, max_length=128):\n",
    "    # Tokenize and encode\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Optimized Toxic Comment Classifier API\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    is_toxic: bool\n",
    "    probability: float\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return RedirectResponse(url=\"/docs\")\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: TextRequest):\n",
    "    try:\n",
    "        # Get BERT embeddings\n",
    "        embeddings = get_bert_embeddings(request.text)\n",
    "        \n",
    "        # Scale features\n",
    "        scaled_features = scaler.transform(embeddings)\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca_features = pca.transform(scaled_features)\n",
    "        \n",
    "        # Predict\n",
    "        probability = best_model.predict_proba(pca_features)[0][1]\n",
    "        prediction = best_model.predict(pca_features)[0]\n",
    "        \n",
    "        return {\n",
    "            \"is_toxic\": bool(prediction),\n",
    "            \"probability\": float(probability)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "'''\n",
    "@app.get(\"/model-info\")\n",
    "async def model_info():\n",
    "    return {\n",
    "        \"best_parameters\": grid_search.best_params_,\n",
    "        \"best_score\": grid_search.best_score_\n",
    "    }\n",
    "'''\n",
    "    \n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "print(\"API is running at http://127.0.0.1:8000\")\n",
    "print(\"Documentation available at http://127.0.0.1:8000/docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
