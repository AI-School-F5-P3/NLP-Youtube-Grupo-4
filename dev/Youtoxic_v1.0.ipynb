{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jvazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data files\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file youtoxic_english_1000.csv to youtoxic dataframe\n",
    "youtoxic = pd.read_csv('youtoxic_english_1000.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\nDont you reckon them 'black lives matter' ba...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\nDont you reckon them 'black lives matter' ba...     True       True   \n",
       "3  There are a very large number of people who do...    False      False   \n",
       "4  The Arab dude is absolutely right, he should h...    False      False   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0     False          False      False         False     False          False   \n",
       "1     False          False      False         False     False          False   \n",
       "2     False          False       True         False     False          False   \n",
       "3     False          False      False         False     False          False   \n",
       "4     False          False      False         False     False          False   \n",
       "\n",
       "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0     False         False            False         False  \n",
       "1     False         False            False         False  \n",
       "2     False         False            False         False  \n",
       "3     False         False            False         False  \n",
       "4     False         False            False         False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtoxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   CommentId        1000 non-null   object\n",
      " 1   VideoId          1000 non-null   object\n",
      " 2   Text             1000 non-null   object\n",
      " 3   IsToxic          1000 non-null   bool  \n",
      " 4   IsAbusive        1000 non-null   bool  \n",
      " 5   IsThreat         1000 non-null   bool  \n",
      " 6   IsProvocative    1000 non-null   bool  \n",
      " 7   IsObscene        1000 non-null   bool  \n",
      " 8   IsHatespeech     1000 non-null   bool  \n",
      " 9   IsRacist         1000 non-null   bool  \n",
      " 10  IsNationalist    1000 non-null   bool  \n",
      " 11  IsSexist         1000 non-null   bool  \n",
      " 12  IsHomophobic     1000 non-null   bool  \n",
      " 13  IsReligiousHate  1000 non-null   bool  \n",
      " 14  IsRadicalism     1000 non-null   bool  \n",
      "dtypes: bool(12), object(3)\n",
      "memory usage: 35.3+ KB\n"
     ]
    }
   ],
   "source": [
    "youtoxic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir Booleanos a Int64 por si alguna librería posterior no puede hacer la transformación intrínseca de boolean a int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   CommentId        1000 non-null   object\n",
      " 1   VideoId          1000 non-null   object\n",
      " 2   Text             1000 non-null   object\n",
      " 3   IsToxic          1000 non-null   int64 \n",
      " 4   IsAbusive        1000 non-null   int64 \n",
      " 5   IsThreat         1000 non-null   int64 \n",
      " 6   IsProvocative    1000 non-null   int64 \n",
      " 7   IsObscene        1000 non-null   int64 \n",
      " 8   IsHatespeech     1000 non-null   int64 \n",
      " 9   IsRacist         1000 non-null   int64 \n",
      " 10  IsNationalist    1000 non-null   int64 \n",
      " 11  IsSexist         1000 non-null   int64 \n",
      " 12  IsHomophobic     1000 non-null   int64 \n",
      " 13  IsReligiousHate  1000 non-null   int64 \n",
      " 14  IsRadicalism     1000 non-null   int64 \n",
      "dtypes: int64(12), object(3)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Identify boolean columns\n",
    "bool_columns = youtoxic.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert boolean columns to int64\n",
    "youtoxic[bool_columns] = youtoxic[bool_columns].astype('int64')\n",
    "\n",
    "# Display the updated information about the dataset to verify the conversion\n",
    "youtoxic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the youtoxic dataframe to an Excel file\n",
    "youtoxic.to_excel('youtoxic.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Después de analizar el fichero en Excel se tienen estas conclusiones:\n",
    "\n",
    "* Efectivamente IsToxic es la bandera que agrupa a los diferentes tipos de clasificaciones de odio.\n",
    "* No hay ningún discurso de odio que no tenga la bandera IsToxic encendida.\n",
    "* No hay ninguna bandera IsToxic encendida sin que haya ninguna de las otras banderas encendidas. Esto significa que la clasificación IsToxic no existe per se y solo representa la existencia de discurso de odio en alguna de las características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje de registros en cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Count of 1s</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IsToxic</td>\n",
       "      <td>462</td>\n",
       "      <td>46.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IsAbusive</td>\n",
       "      <td>353</td>\n",
       "      <td>35.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IsThreat</td>\n",
       "      <td>21</td>\n",
       "      <td>2.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IsProvocative</td>\n",
       "      <td>161</td>\n",
       "      <td>16.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IsObscene</td>\n",
       "      <td>100</td>\n",
       "      <td>10.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IsHatespeech</td>\n",
       "      <td>138</td>\n",
       "      <td>13.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IsRacist</td>\n",
       "      <td>125</td>\n",
       "      <td>12.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IsNationalist</td>\n",
       "      <td>8</td>\n",
       "      <td>0.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IsSexist</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IsHomophobic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IsReligiousHate</td>\n",
       "      <td>12</td>\n",
       "      <td>1.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IsRadicalism</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Column  Count of 1s Percentage\n",
       "0           IsToxic          462     46.20%\n",
       "1         IsAbusive          353     35.30%\n",
       "2          IsThreat           21      2.10%\n",
       "3     IsProvocative          161     16.10%\n",
       "4         IsObscene          100     10.00%\n",
       "5      IsHatespeech          138     13.80%\n",
       "6          IsRacist          125     12.50%\n",
       "7     IsNationalist            8      0.80%\n",
       "8          IsSexist            1      0.10%\n",
       "9      IsHomophobic            0      0.00%\n",
       "10  IsReligiousHate           12      1.20%\n",
       "11     IsRadicalism            0      0.00%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get total number of rows\n",
    "total_rows = len(youtoxic)\n",
    "\n",
    "# Select integer columns\n",
    "int_cols = youtoxic.select_dtypes(include=['int64']).columns\n",
    "\n",
    "# Calculate counts and percentages\n",
    "results = []\n",
    "for col in int_cols:\n",
    "    count_ones = youtoxic[col].sum()  # Sum of 1s\n",
    "    percentage = (count_ones / total_rows) * 100\n",
    "    results.append({\n",
    "        'Column': col,\n",
    "        'Count of 1s': count_ones,\n",
    "        'Percentage': f'{percentage:.2f}%'\n",
    "    })\n",
    "\n",
    "# Create and display table\n",
    "table = pd.DataFrame(results)\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: IsToxic es la columna bandera que indica si hay algún comentario de odio en las otras 11 categorias. No se necesita crear una columna que sumarice 12 columnas porque esa columna ya está en el dataset.\n",
    "\n",
    "* Los datos no nos permiten clasificar un discurso por medio de las características: - \n",
    "- IsHomophobic\n",
    "- IsRadicalism\n",
    "\n",
    "* Las características \n",
    "- IsSexist\n",
    "- IsNationalist\n",
    "- IsReligiousHate\n",
    "- IsThreat \n",
    "están severamente desbalanceadas.\n",
    "\n",
    "* También están muy desbalanceadas las características: \n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "* La única característica donde el desbalanceo es razonable es:\n",
    "- IsAbusive\n",
    "\n",
    "* Con estas observaciones se presume que el modelo con una sola bandera IsToxic es muy generalista y que por lo tanto no es capaz de predecir con una mejor precisión (alrededor de 70%) si el mensaje es de odio o no.\n",
    "\n",
    "* Un modelo multi-label binary classification (entiendo que con un Naive Bayes) puede ser una mejor solución en este caso, utilizando solamente las características:\n",
    "- IsAbusive\n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "Por supuesto, resolviendo el problema del desbalanceo en las últimas 4 características.\n",
    "\n",
    "En cualquier caso sí es importante utilizar la característica VideoId como información útil para el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A partir de aquí se preparan 2 datasets. Uno para entrenar un modelo multi-etiqueta de clasificación binaria. El otro para un modelo de una sola categoria: Istoxic\n",
    "\n",
    "Youmultihatred:\n",
    "- VideoId\n",
    "- Text\n",
    "- IsAbusive\n",
    "- IsProvocative\n",
    "- IsObscene\n",
    "- IsHateSpeech\n",
    "- IsRacist\n",
    "\n",
    "Youtoxic:\n",
    "- VideoId\n",
    "- Text\n",
    "- IsToxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep for version without multi-label classification\n",
    "\n",
    "youmultihatred = youtoxic[['VideoId', 'Text', 'IsAbusive','IsProvocative','IsObscene','IsHatespeech', 'IsRacist']]\n",
    "\n",
    "youtoxic = youtoxic[['VideoId', 'Text', 'IsToxic']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess dataset\n",
    "\n",
    "1. Remover URLs\n",
    "2. Remover special characters y números\n",
    "3. Convertir a minúsculas\n",
    "4. Remover espacios innecesarios\n",
    "Tokenizar momentaneamente para:\n",
    "5. Quitar Stopwords\n",
    "6. Lematizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>people would take step back make case wasnt an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>dont reckon black life matter banner held whit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>large number people like police officer called...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       VideoId                                               Text  IsAbusive  \\\n",
       "0  04kJtp6pVXI  people would take step back make case wasnt an...          0   \n",
       "1  04kJtp6pVXI  law enforcement trained shoot apprehend traine...          1   \n",
       "2  04kJtp6pVXI  dont reckon black life matter banner held whit...          1   \n",
       "3  04kJtp6pVXI  large number people like police officer called...          0   \n",
       "4  04kJtp6pVXI  arab dude absolutely right shot extra time sho...          0   \n",
       "\n",
       "   IsProvocative  IsObscene  IsHatespeech  IsRacist  \n",
       "0              0          0             0         0  \n",
       "1              0          0             0         0  \n",
       "2              0          1             0         0  \n",
       "3              0          0             0         0  \n",
       "4              0          0             0         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# No utilizaremos textblob dado que las metricas mejoraron al no utilizarlo.\n",
    "# Ademas, textblob es muy lento para procesar grandes cantidades de texto.\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\@w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Correct misspellings\n",
    "    #text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the preprocess_text function to the 'Text' column of the youtoxic and youmultihatred dataframes\n",
    "youtoxic['Text'] = youtoxic['Text'].apply(preprocess_text)\n",
    "youmultihatred['Text'] = youmultihatred['Text'].apply(preprocess_text)\n",
    "\n",
    "# Display the updated dataframes\n",
    "youtoxic.head()\n",
    "youmultihatred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sample csv file from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a random selection of 20 rows from youtoxic dataframe to a CSV file\n",
    "# Uncomment the line below to export the sample\n",
    "\n",
    "youtoxic.head(n=20).to_csv('youtoxic_sample.csv', index=False)\n",
    "youmultihatred.head(n=20).to_csv('youmultihatred_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Modelo de ML utilizando solo la característica Text. IsToxic es la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.93\n",
      "Test Accuracy: 0.69\n",
      "Overfitting Percentage: 25.57%\n",
      "Precision: 0.80\n",
      "Recall: 0.57\n",
      "F1-score: 0.67\n",
      "Best hyperparameters: {'C': 10, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.69\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Preprocess the text data\n",
    "X = youtoxic['Text']\n",
    "y = youtoxic['IsToxic']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = model.predict(X_train_tfidf)\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate overfitting percentage\n",
    "overfitting_percentage = ((train_accuracy - test_accuracy) / train_accuracy) * 100\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Overfitting Percentage: {overfitting_percentage:.2f}%')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n",
    "\n",
    "\n",
    "# Optimize hyperparameters using cross-validation\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Modelo de ML utilizando las características Text y VideoId. IsToxic es el target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.93\n",
      "Test Accuracy: 0.71\n",
      "Overfitting Percentage: 23.24%\n",
      "Precision: 0.81\n",
      "Recall: 0.60\n",
      "F1-score: 0.69\n",
      "Best hyperparameters: {'C': 10, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.69\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess the text data\n",
    "X_text = youtoxic['Text']\n",
    "X_video_id = youtoxic['VideoId']\n",
    "y = youtoxic['IsToxic']\n",
    "\n",
    "# Encode the video IDs\n",
    "label_encoder = LabelEncoder()\n",
    "X_video_id_encoded = label_encoder.fit_transform(X_video_id)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_text_train, X_text_test, X_video_id_train, X_video_id_test, y_train, y_test = train_test_split(X_text, X_video_id_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_text_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = vectorizer.transform(X_text_test)\n",
    "\n",
    "# Combine text and video ID features\n",
    "X_train = np.hstack((X_text_train_tfidf.toarray(), X_video_id_train.reshape(-1, 1)))\n",
    "X_test = np.hstack((X_text_test_tfidf.toarray(), X_video_id_test.reshape(-1, 1)))\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate overfitting percentage\n",
    "overfitting_percentage = ((train_accuracy - test_accuracy) / train_accuracy) * 100\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Overfitting Percentage: {overfitting_percentage:.2f}%')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')\n",
    "\n",
    "\n",
    "# Optimize hyperparameters using cross-validation\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Modelo de Machine Learning de clasificación binaria de multi etiquetas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "--------------------------------------------------\n",
      "Training Accuracy: 0.8825\n",
      "Test Accuracy: 0.8070\n",
      "Overfitting Percentage: 8.56%\n",
      "Cross-validation Scores: [0.56875 0.5375  0.6125  0.54375 0.5625 ]\n",
      "Mean CV Score: 0.5650 (+/- 0.0528)\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    IsAbusive       0.92      0.14      0.25        77\n",
      "IsProvocative       0.00      0.00      0.00        38\n",
      "    IsObscene       0.00      0.00      0.00        23\n",
      " IsHatespeech       0.00      0.00      0.00        35\n",
      "     IsRacist       0.00      0.00      0.00        30\n",
      "\n",
      "    micro avg       0.92      0.05      0.10       203\n",
      "    macro avg       0.18      0.03      0.05       203\n",
      " weighted avg       0.35      0.05      0.09       203\n",
      "  samples avg       0.06      0.04      0.04       203\n",
      "\n",
      "Hamming Loss: 0.1930\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class HateSpeechClassifier:\n",
    "    def __init__(self):\n",
    "        # Initialize encoders and vectorizer\n",
    "        self.video_id_encoder = LabelEncoder()\n",
    "        self.tfidf = TfidfVectorizer(max_features=5000)\n",
    "        self.thresholds = {\n",
    "            'IsAbusive': 0.5,\n",
    "            'IsProvocative': 0.5,\n",
    "            'IsObscene': 0.5,\n",
    "            'IsHatespeech': 0.5,\n",
    "            'IsRacist': 0.5\n",
    "        }\n",
    "        \n",
    "\n",
    "    def prepare_features(self, df, fit_tfidf=True):\n",
    "        \"\"\"Prepare features from input dataframe\"\"\"\n",
    "        # Encode VideoId\n",
    "        video_ids_encoded = self.video_id_encoder.fit_transform(df['VideoId'])\n",
    "        \n",
    "        # Create TF-IDF features\n",
    "        if fit_tfidf:\n",
    "            text_features = self.tfidf.fit_transform(df['Text'])\n",
    "        else:\n",
    "            text_features = self.tfidf.transform(df['Text'])\n",
    "        \n",
    "        # Combine features\n",
    "        return np.hstack((\n",
    "            video_ids_encoded.reshape(-1, 1),\n",
    "            text_features.toarray()\n",
    "        ))\n",
    "\n",
    "    def prepare_features_predict(self, df):\n",
    "        \"\"\"Prepare features from input dataframe for prediction\"\"\"\n",
    "        # Encode VideoId\n",
    "        X = self.prepare_features_predict(df)\n",
    "        \n",
    "        # Create TF-IDF features\n",
    "        text_features = self.tfidf.transform(df['Text'])\n",
    "        \n",
    "        # Combine features\n",
    "        return np.hstack((\n",
    "            video_ids_encoded.reshape(-1, 1),\n",
    "            text_features.toarray()\n",
    "        ))\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"Train the multi-label classification model\"\"\"\n",
    "        # Prepare features\n",
    "        X = self.prepare_features(df)\n",
    "        \n",
    "        # Prepare target variables\n",
    "        y = df[['IsAbusive', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist']].values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Define base classifier and parameter grid for optimization\n",
    "        base_classifier = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'estimator__n_estimators': [100, 200],\n",
    "            'estimator__max_depth': [10, 20],\n",
    "            'estimator__min_samples_split': [2, 5],\n",
    "            'estimator__min_samples_leaf': [1, 2]\n",
    "        }\n",
    "\n",
    "        # Create multi-output classifier\n",
    "        self.classifier = MultiOutputClassifier(base_classifier)\n",
    "\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            self.classifier,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.classifier = grid_search.best_estimator_\n",
    "\n",
    "        # Calculate and display metrics\n",
    "        self._evaluate_model(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _evaluate_model(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Evaluate model performance and display metrics\"\"\"\n",
    "        # Training predictions\n",
    "        y_train_pred = self.classifier.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train.flatten(), y_train_pred.flatten())\n",
    "\n",
    "        # Test predictions\n",
    "        y_test_pred = self.classifier.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test.flatten(), y_test_pred.flatten())\n",
    "\n",
    "        # Calculate overfitting percentage\n",
    "        overfitting_percentage = ((train_accuracy - test_accuracy) / train_accuracy) * 100\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(self.classifier, X_train, y_train, cv=5)\n",
    "\n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Overfitting Percentage: {overfitting_percentage:.2f}%\")\n",
    "        print(f\"Cross-validation Scores: {cv_scores}\")\n",
    "        print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_test_pred, \n",
    "              target_names=['IsAbusive', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist']))\n",
    "        print(f\"Hamming Loss: {hamming_loss(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        # Prepare features\n",
    "        X = self.prepare_features(df)\n",
    "        \n",
    "        # Get raw predictions\n",
    "        raw_predictions = self.classifier.predict_proba(X)\n",
    "        \n",
    "        # Apply thresholds and combine results\n",
    "        final_predictions = []\n",
    "        for sample_predictions in zip(*raw_predictions):\n",
    "            label_predictions = {}\n",
    "            is_hatred = False\n",
    "            \n",
    "            for idx, (label, pred_probs) in enumerate(zip(\n",
    "                ['IsAbusive', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist'],\n",
    "                sample_predictions\n",
    "            )):\n",
    "                # Get probability of positive class\n",
    "                pos_prob = pred_probs[1]\n",
    "                # Apply threshold\n",
    "                is_positive = pos_prob >= self.thresholds[label]\n",
    "                label_predictions[label] = is_positive\n",
    "                \n",
    "                # If any label is positive, mark as hatred\n",
    "                if is_positive:\n",
    "                    is_hatred = True\n",
    "                    \n",
    "            label_predictions['IsHatred'] = is_hatred\n",
    "            final_predictions.append(label_predictions)\n",
    "            \n",
    "        return final_predictions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = youmultihatred\n",
    "\n",
    "    # Initialize and train classifier\n",
    "    classifier = HateSpeechClassifier()\n",
    "    classifier.train(df)\n",
    "    \n",
    "    # Make predictions on sample data\n",
    "    sample_predictions = classifier.predict(df)\n",
    "    \n",
    "    '''\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i, pred in enumerate(sample_predictions):\n",
    "        print(f\"\\nText {i+1} Predictions:\")\n",
    "        for label, value in pred.items():\n",
    "            print(f\"{label}: {value}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building an API to consume the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [7204]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52271 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:52271 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [7204]\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create a FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with specific origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define the request model\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str\n",
    "    video_id: int\n",
    "\n",
    "@app.post('/predict')\n",
    "async def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        text_tfidf = vectorizer.transform([request.text])\n",
    "        input_data = np.hstack((text_tfidf.toarray(), [[request.video_id]]))\n",
    "        prediction = model.predict(input_data)[0]\n",
    "        return {'is_toxic': bool(prediction)}\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Add a health check endpoint\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {'status': 'ok'}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host='127.0.0.1', port=8000)  # Changed host to localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
